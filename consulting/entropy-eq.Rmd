---
title: "Entropy EQ"
author: "Zhenya & Segah"
date: "12/20/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Entropy gleans its insights from deep NLU (subtopic in NLP) analysis of written communication.

In its simplified form, this is an expanded version of Maximum likelihood model called Max Entropy:<br/>
<center><img width="300" src="https://user-images.githubusercontent.com/1756903/32816722-17b7e470-c96f-11e7-8225-9ee0e0882343.png"></center>

## R Examples:
```r
library(entropy)
languageML('Hello, we are on a mission to change written communication.')
```

```r
## [1] "Result"        "driven"        "Passive voice" "Concise"
## [5] "Informal"      "Thoughtful"    "Assertive"
```

```r
languageML("Don't get us wrong, we love existing tools such as Gmail and Slack. Yet, we believe that our communication is poor (Emojis - come on...). We are capable of much more!")
```

```r
## [1] "Thoughtful" "Informal"   "Verbose"
```

## Architecture

<img width="700" src="images/entropy architecture.svg">

## Demo

Please reach out to us: [entropy@caura.co](mailto:entropy@caura.co)
<center>[![](entropy.gif)](mailto:entropy@caura.co?subject=demo)</center>

<br>
<br>

> Places where linguists traditionally look to see [entropy] are not where the fundamentals of language are.
>
> -- <cite>Marcelo A. Montemurro and Dami√°n H. Zanette</cite>
